---
title: 'CityLearn experiment'
authors: [zqqqj]
tags: [RL]
date: 2025-10-27
---

## ä»£ç è·¯å¾„ï¼šhttps://github.com/zqqqqqqj1110/CityLearn_exp.git

# target

æ¯”è¾ƒä¸åŒä»»åŠ¡é…ç½®ä¸‹ï¼ŒRBCï¼ˆRule-Based Controlï¼‰ä¸ RLCï¼ˆReinforcement Learning Controlï¼‰åœ¨èƒ½æºã€æ’æ”¾ã€å³°å€¼ä¸èˆ’é€‚åº¦æ–¹é¢çš„è¡¨ç°ã€‚
æ•°é‡ï¼š å…± 17 ä¸ªä»»åŠ¡ï¼ˆå•å»ºç­‘ã€å¤šå»ºç­‘ã€å•ç›®æ ‡ã€å¤šç›®æ ‡ï¼‰
è¾“å‡ºæŒ‡æ ‡ï¼š

1. Electricity Cost [$]
2. Emissions [kg COâ‚‚]
3. Peak Demand [kW]
4. Discomfort Penalty [â€“]

# å‡†å¤‡ç¯å¢ƒ

åˆ›å»ºç¯å¢ƒï¼Œå‡†å¤‡ citylearn çš„ conda env

```
conda create -n citylearn python=3.10 -y
conda activate citylearn
```

å®‰è£… city learn V2

```
git clone https://github.com/intelligent-environments-lab/CityLearn.git
cd CityLearn
pip install -e .    # æºç æ§åˆ¶ç‰ˆï¼Œä»…ç”¨äºå¤ç°
pip install citylearn   # åç»­è¯•éªŒå¯ä»¥ç›´æ¥ç”¨è¿™ä¸ªä¸‹è½½
```

<!-- truncate -->

å¦‚æœæ˜¯ macOSï¼Œpip install çš„æ—¶å€™ä¼šå‘ç”Ÿå¦‚ä¸‹æŠ¥é”™ï¼š

```
ERROR: Ignored the following versions that require a different python version: 2.3.0 Requires-Python >=3.11; 2.3.1 Requires-Python >=3.11; 2.3.2 Requires-Python >=3.11; 2.3.3 Requires-Python >=3.11; 2.3.4 Requires-Python >=3.11
ERROR: Could not find a version that satisfies the requirement openstudio<=3.3.0 (from citylearn) (from versions: 3.5.0, 3.5.1, 3.6.0, 3.6.1, 3.7.0rc1, 3.7.0, 3.8.0, 3.9.0, 3.10.0rc5, 3.10.0)
ERROR: No matching distribution found for openstudio<=3.3.0
```

è¿™æ˜¯å› ä¸º OpenStudio åªæ”¯æŒ x86 æ¶æ„ï¼ŒApple Silicon (arm64) ä¸‹æ²¡æœ‰ç¼–è¯‘ç‰ˆæœ¬ï¼Œè·³è¿‡ä¾èµ–ï¼Œè‡ªå·±æ‰‹åŠ¨å®‰è£…

```
pip install -e . --no-deps
pip install numpy==1.26.4 pandas pyyaml scikit-learn==1.2.2 simplejson torch torchvision gymnasium==0.28.1 nrel-pysam doe_xstock
```

è¿™è¾¹å»ºè®®è¿˜æ˜¯ç”¨ linux æ¯”è¾ƒå¥½ï¼Œåç»­å¤ç°éƒ½åŸºäº Linux è€Œé macOSï¼ŒmacOS çš„ç¯å¢ƒé…ç½®ç­‰æˆ‘æœ‰ç©ºäº†å†å†™ï¼ˆæ¯•ç«Ÿæœ¬åœ°è¿è¡Œæœ€æ–¹ä¾¿ï¼‰

ç›®å‰ macOS å¤±è´¥çš„åŸå› ï¼š**OpenStudio ä¸æ”¯æŒ macOS ARM æ¶æ„**

# è¿›è¡Œæµ‹è¯•

é…ç½®å®Œç¯å¢ƒä¹‹åï¼Œå¯ä»¥ä½¿ç”¨æµ‹è¯•ä»£ç çœ‹çœ‹æ•ˆæœ

```
from citylearn.citylearn import CityLearnEnv
from pathlib import Path
import numpy as np

# åŠ è½½ CityLearn ç¯å¢ƒ
schema = Path('/root/CityLearn/data/datasets/citylearn_challenge_2021/schema.json')
env = CityLearnEnv(schema=schema)
print("âœ… Env loaded successfully!")

# é‡ç½®ç¯å¢ƒï¼ˆGymnasium é£æ ¼ï¼‰
obs, info = env.reset()

# æ‰“å°å»ºç­‘æ•°é‡ä¸ID
print("âœ… Number of buildings:", len(env.buildings))

# æ‰“å°æ¯æ ‹å»ºç­‘çš„è§‚æµ‹é•¿åº¦
obs_lens = [len(o) for o in obs]
print("âœ… Observation lengths per building:", obs_lens)

# ç¤ºä¾‹ï¼šæŸ¥çœ‹ç¬¬ä¸€ä¸ªå»ºç­‘çš„å‰5ä¸ªç‰¹å¾
print("âœ… Sample observation (Building 1):", obs[0][:5])

# å¦‚æœè¦è½¬æ¢ä¸ºçŸ©é˜µï¼Œå¯ä»¥å…ˆå¯¹é½é•¿åº¦ï¼ˆå–æœ€çŸ­é•¿åº¦æˆ–è¡¥é›¶ï¼‰
min_len = min(obs_lens)
obs_array = np.array([o[:min_len] for o in obs])
print("âœ… Converted obs array shape:", obs_array.shape)
```

æœ‰äº”ä¸ª âœ… å°± ok äº†ï¼Œè¾“å‡ºç»“æœåº”è¯¥å¦‚ä¸‹ï¼š

```
(citylearnv2) root@autodl-container-491e4b9a34-9e5b11d5:~/CityLearn# python Untitled1.py
Couldn't import dot_parser, loading of dot files will not be possible.
âœ… Env loaded successfully!
âœ… Number of buildings: 9
âœ… Observation lengths per building: [28, 27, 26, 27, 28, 28, 27, 27, 27]
âœ… Sample observation (Building 1): [1, 4, 1, 9.4, 10.020000457763672]
âœ… Converted obs array shape: (9, 26)
```

number of buildings å¯ä»¥ç†è§£ä¸º agent çš„æ•°é‡ï¼Œæ¯ä¸€ä¸ª agent éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„æ™ºèƒ½ä½“ï¼Œå…±äº«å¤©æ°”ã€ç”µä»·ã€ç¢³æ’æ”¾ç­‰å…¨å±€ä¿¡æ¯ï¼Œå…±åŒå­¦ä¹ åè°ƒç­–ç•¥
Observation lengths per building æ˜¯æ¯ä¸ª agent çš„ç‰¹å¾æ•°é‡ï¼Œè¿™äº›ç‰¹å¾å¯ä»¥æ˜¯
å½“å‰ç”µä»·ï¼ŒCOâ‚‚ å¼ºåº¦ï¼Œå½“å‰æ—¶åˆ»ï¼Œè®¾å¤‡èƒ½è€—ï¼ˆDHWã€TESã€Batteryï¼‰ ç­‰
Sample observation æŠŠä¸€æ ‹å»ºç­‘çš„ç‰¹å¾ç¼–ç æˆä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼Œå…·ä½“å«ä¹‰å–å†³äº schema çš„é…ç½®ï¼ˆobservations å­—æ®µï¼‰ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
![](http://8.130.141.48/wp-content/uploads/2025/11/17616169599159-scaled.jpg)
Converted obs array shape æ˜¯ä¼ å…¥åˆ°æ¨¡å‹çš„æ•°ç»„ï¼Œæ˜¯ä¸€ä¸ªå¤§æ•´ä½“ï¼Œç”¨äºè®­ç»ƒ

# Control task distribution

è¯¥ç« èŠ‚è®¾è®¡äº† 17 ä¸ªä¸åŒçš„æ§åˆ¶ä»»åŠ¡ï¼Œå¯¹æ¯” 3 ç§æ§åˆ¶æ–¹å¼ + 4 ç§æ§åˆ¶ç›®æ ‡ã€‚ä¸‰ç§æ–¹æ³•åœ¨ä»£ç ä¸­çš„æœ¬è´¨åŒºåˆ«å°±æ˜¯ reward ä¸åŒï¼Œéœ€è¦åœ¨ä¼ å…¥ CityLearnEnv() æ—¶ï¼Œè‡ªå®šä¹‰ Reward Function
| æ§åˆ¶å™¨ | ç®€ä»‹ | ç±»å‹ |
| ------------------------------------------- | ----------------- | ---- |
| **Baseline** | æ— æ§åˆ¶ï¼Œç›´æ¥è·Ÿéšè´Ÿè·æ›²çº¿ | è¢«åŠ¨ |
| **RBC (rule-based controller)** | æ‰‹å·¥è®¾å®šè§„åˆ™ï¼Œä¾‹å¦‚â€œç”µä»·ä½æ—¶å……ç”µâ€ | åŸºå‡†æ§åˆ¶ |
| **RLC (reinforcement learning controller)** | SAC ç®—æ³•è‡ªåŠ¨å­¦ä¹ ç­–ç•¥ | å¼ºåŒ–å­¦ä¹  |

## 1. Cost or Emission Reduction

ä»¥æœ€å°åŒ–ç”µè´¹æˆæœ¬æˆ–ç¢³æ’æ”¾æ€»é‡ä¸º KPIï¼Œè¿›è¡Œä¸‰ç§æ–¹å¼çš„è®­ç»ƒ

### 1.1 å®éªŒè®¾ç½®

æ•°æ®é›†ï¼š CityLearn Challenge 2023 (Phase 2 local evaluation)
å»ºç­‘ï¼š 9 æ ‹ä½å®…
æ§åˆ¶å¯¹è±¡ï¼š DHW (çƒ­æ°´å‚¨èƒ½) æˆ– BESS + PV (ç”µæ± å‚¨èƒ½+å…‰ä¼)
æ§åˆ¶å˜é‡ï¼š å‚¨èƒ½å……æ”¾ç”µåŠŸç‡

### 1.2 ç¯å¢ƒ

Baseline â€“ ä¸åšä»»ä½•æ§åˆ¶ã€‚
RBC â€“ æ‰‹åŠ¨è§„åˆ™ï¼š
ç”µä»·ä½ â†’ å……èƒ½ï¼›
ç”µä»·é«˜ â†’ æ”¾èƒ½ã€‚
SAC â€“ æ·±åº¦å¼ºåŒ–å­¦ä¹ è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜è°ƒåº¦

### 1.3 ä»£ç 

```
from citylearn.citylearn import CityLearnEnv
from citylearn.agents.base import BaselineAgent
from citylearn.agents.rbc import BasicRBC
from citylearn.agents.sac import SAC
from pathlib import Path

def run(agent_class, schema, episodes=1, central_agent=True, save_dir=None, **kwargs):
    print(f"\n=== Running {agent_class.__name__} ===")
    env = CityLearnEnv(schema, central_agent=central_agent)
    agent = agent_class(env)
    agent.learn(episodes=episodes, deterministic_finish=True)
    results = env.evaluate()
    env.close()
    print(f"âœ… {agent_class.__name__} finished.")
    if save_dir:
        Path(save_dir).mkdir(parents=True, exist_ok=True)
        (Path(save_dir)/"metrics.txt").write_text(str(results))
    return results

def main():
    # ä½¿ç”¨æœ¬åœ°æ•°æ®é›† schema
    schema = "data/datasets/citylearn_challenge_2023_phase_2_local_evaluation/schema.json"

    # 1 Baseline æ— æ§åˆ¶
    res_base = run(BaselineAgent, schema, episodes=1, central_agent=True, save_dir="results/4_1_1_baseline")

    # 2 RBC è§„åˆ™æ§åˆ¶
    res_rbc = run(BasicRBC, schema, episodes=1, central_agent=True, save_dir="results/4_1_1_rbc")

    # 3 SAC å¼ºåŒ–å­¦ä¹ æ§åˆ¶
    res_sac = run(SAC, schema, episodes=5, central_agent=False, save_dir="results/4_1_1_sac")

    # è¾“å‡ºå¯¹æ¯”ç»“æœ
    print("\n=== Comparison ===")
    print("Baseline:", res_base)
    print("RBC:", res_rbc)
    print("SAC:", res_sac)

if __name__ == "__main__":
    main()
```

ç›®çš„æ˜¯åœ¨ç›¸åŒç¯å¢ƒä¸‹ï¼Œä¾æ¬¡è¿è¡Œä¸‰ç§æ§åˆ¶ç­–ç•¥ï¼ˆBaseline â†’ RBC â†’ SACï¼‰ï¼Œå¹¶ä¿å­˜ç»“æœã€‚ä¸»ä½“æ˜¯ run å‡½æ•°ï¼Œå°±è¯¦ç»†è®²ä»–äº†
run å‡½æ•°æ˜¯ä¸€ä¸ªé€šç”¨çš„å®éªŒè¿è¡Œå‡½æ•°ï¼Œåˆ†ä¸ºä¸€ä¸‹æ­¥éª¤

1. åˆå§‹åŒ–ç¯å¢ƒ

```
env = CityLearnEnv(schema, central_agent=central_agent)
```

CityLearnEnv æ˜¯ CityLearn çš„æ ¸å¿ƒç¯å¢ƒç±»ï¼Œå®ƒæ ¹æ® schema.json è¯»å–åŸå¸‚èƒ½è€—æ•°æ®ï¼ˆå»ºç­‘è´Ÿè·ã€ç”µä»·ã€å¤ªé˜³èƒ½ã€å¤©æ°”ç­‰ï¼‰ã€‚
å‚æ•°ï¼š
schema: å®šä¹‰æ•°æ®æ¥æºä¸ç¯å¢ƒé…ç½®ï¼ˆç›¸å½“äºä¸€ä¸ªé…ç½®æ–‡ä»¶è·¯å¾„ï¼‰
central_agent: æ˜¯å¦ä¸ºé›†ä¸­å¼æ§åˆ¶ï¼ˆTrue è¡¨ç¤ºä¸€ä¸ªæ™ºèƒ½ä½“æ§åˆ¶æ‰€æœ‰å»ºç­‘ï¼‰

2. åˆå§‹åŒ–æ™ºèƒ½ä½“

```
agent = agent_class(env)
```

BaselineAgentï¼šå®Œå…¨ä¸åšæ§åˆ¶ï¼ˆä»…è¢«åŠ¨æ‰§è¡Œç¯å¢ƒåŠ¨ä½œï¼‰
BasicRBCï¼šåŸºäºè§„åˆ™çš„æ§åˆ¶ï¼ˆRule-Based Controlï¼‰
SACï¼šå¼ºåŒ–å­¦ä¹ ç®—æ³• Soft Actor-Critic æ§åˆ¶å™¨

3. è®­ç»ƒæ™ºèƒ½ä½“

```
agent.learn(episodes=episodes, deterministic_finish=True)
```

episodesï¼šè®­ç»ƒè½®æ•°ï¼ˆä½ ç»™ Baseline/RBC å„ 1 æ¬¡ï¼ŒSAC ç»™ 5 æ¬¡ï¼‰
deterministic_finish=Trueï¼šåœ¨æœ€åä¸€ä¸ª episode ä»¥ç¡®å®šæ€§æ–¹å¼ç»“æŸï¼ˆç”¨äºè¯„ä¼°)

4. è¯„ä¼°ç»“æœ

```
results = env.evaluate()
```

è¿”å›ä¸€ä¸ªæ€§èƒ½æŒ‡æ ‡å­—å…¸ï¼Œå¦‚èƒ½è€—ã€å³°å€¼è´Ÿè·ã€ç¢³æ’æ”¾ã€æˆæœ¬ç­‰

5. æ‰“å°ä¸ä¿å­˜

### 1.4 é—®é¢˜æ±‡æ€»

#### 1.4.1 RBCï¼ˆè§„åˆ™æ§åˆ¶ï¼‰çš„è§„åˆ™æ˜¯ä»€ä¹ˆï¼Ÿä»£ç ä¸­æ€ä¹ˆä½“ç°ï¼Ÿ

RBCï¼ˆRule-Based Controlï¼‰æ˜¯ åŸºäºäººå·¥ç»éªŒè§„åˆ™çš„æ§åˆ¶ç­–ç•¥ã€‚åœ¨ CityLearn ä¸­ï¼ŒRBC ä¸»è¦è´Ÿè´£æ§åˆ¶æ¯æ ‹å»ºç­‘çš„å‚¨èƒ½è®¾å¤‡ï¼ˆç”µæ± ï¼‰ä½•æ—¶å……ç”µã€ä½•æ—¶æ”¾ç”µã€‚
é»˜è®¤è§„åˆ™å¦‚ä¸‹ï¼š
å½“ç”µä»·ä½ï¼ˆoff-peakï¼‰æ—¶ â†’ å……ç”µ
å½“ç”µä»·é«˜ï¼ˆpeakï¼‰æ—¶ â†’ æ”¾ç”µ
è‹¥ç”µæ± æ»¡æˆ–ç©º â†’ ä¸åŠ¨ä½œ
é€»è¾‘å®šä¹‰åœ¨ city learn åŒ…çš„è¯¥æ–‡ä»¶ä¸­ï¼šcitylearn/agents/rbc.pyã€‚ç®€å•çš„æ¥è¯´ï¼Œå°±æ˜¯ä¸€ä¸ª if else è€Œå·²

#### 1.4.2. SACï¼ˆå¼ºåŒ–å­¦ä¹ æ§åˆ¶ï¼‰ç”¨çš„æ˜¯ä»€ä¹ˆç®—æ³•ï¼Ÿ

SAC æ˜¯ Soft Actor-Criticï¼‰ç®—æ³•ï¼ŒåŸºäºæœ€å¤§ç†µå¼ºåŒ–å­¦ä¹ ï¼ˆMaximum Entropy RLï¼‰çš„è¿ç»­åŠ¨ä½œæ§åˆ¶ç®—æ³•ã€‚å°±è·Ÿæµç¨‹å›¾ä¸­çš„ä¸€æ ·ï¼Œæœ‰ä¸‰éƒ¨åˆ†æ„æˆ
| æ¨¡å— | ç±»å‹ | åŠŸèƒ½ |
| ----------------- | ---- | ------ |
| Actor | ç­–ç•¥ç½‘ç»œ | ç”ŸæˆåŠ¨ä½œåˆ†å¸ƒ |
| Criticâ‚ / Criticâ‚‚ | Q ç½‘ç»œ | è¯„ä¼°åŠ¨ä½œä»·å€¼ |
| Target Critic | ç›®æ ‡ç½‘ç»œ | ç¨³å®šè®­ç»ƒ |

#### 1.4.3. RBC è§„åˆ™èƒ½å¦è‡ªå®šä¹‰

BasicRBC æ˜¯ CityLearn è‡ªå¸¦çš„é»˜è®¤è§„åˆ™æ§åˆ¶å™¨ï¼ˆå®šä¹‰åœ¨ rbc.pyï¼‰ã€‚ä½†å®Œå…¨å¯ä»¥è‡ªå®šä¹‰è‡ªå·±çš„è§„åˆ™ã€‚åªéœ€ç»§æ‰¿åŸºç±»å¹¶é‡å†™ select_actions()å³å¯
ä¾‹ï¼š

```
from citylearn.agents.base import BaseAgent

class MyRBC(BaseAgent):
    def select_actions(self, observations):
        actions = []
        for obs in observations:
            price = obs['electricity_price']
            soc = obs['storage_soc']
            solar = obs['solar_generation']
            if price < 0.05 and solar > 0:
                action = 1.0      # åŠ å¼ºå……ç”µ
            elif price > 0.2 or soc > 0.8:
                action = -1.0     # æ”¾ç”µ
            else:
                action = 0.0
            actions.append(action)
        return actions

run(MyRBC, schema, episodes=1, central_agent=True)
```

#### 1.4.4. SAC å¼ºåŒ–å­¦ä¹ æ§åˆ¶æ˜¯å¦å’Œé—®é¢˜ 3 ä¸€æ ·ï¼Ÿ

ä¹Ÿå¯ä»¥å’Œé—®é¢˜ 3 ä¸€æ ·ï¼Œè‡ªå®šä¹‰ï¼Œåªè¦å®ç°ç›¸åŒæ¥å£å³å¯
ä¾‹ï¼š

```
from citylearn.agents.base import BaseAgent
from stable_baselines3 import PPO

class PPOAgent(BaseAgent):
    def __init__(self, env):
        super().__init__(env)
        self.model = PPO('MlpPolicy', env, verbose=1)

    def learn(self, episodes=1, **kwargs):
        self.model.learn(total_timesteps=episodes * len(self.env.time_steps))

    def select_actions(self, observations):
        action, _ = self.model.predict(observations, deterministic=True)
        return action

run(PPOAgent, schema, episodes=5, central_agent=False)
```

psï¼šschema æ–‡ä»¶ä¸­ reward_function é…ç½®ï¼ˆå…·ä½“åœ¨ 2.2 ä¸­è§£é‡Šï¼‰

```
"reward_function": {
  "type": "citylearn.reward_function.RewardFunction",
  "attributes": {
    "electricity_cost_weight": 0.6,
    "emission_weight": 0.4,
    "peak_to_average_ratio_weight": 0.0
  }
}
```

## 2. Peak Reduction

ä»¥å‰Šå‡ç¤¾åŒºæ€»åŠŸç‡å³°å€¼ä¸º KPIï¼Œè¿›è¡Œä¸‰ç§æ–¹å¼çš„è®­ç»ƒ

### 2.1 ç¯å¢ƒ

RBC ä»æŒ‰è§„åˆ™ä¸Šè¿°è¿è¡Œ
SAC è‡ªåŠ¨å­¦ä¼šé”™å³°æ“ä½œâ€”â€”è®©å„å»ºç­‘åœ¨ä¸åŒæ—¶é—´æ”¾ç”µã€‚

### 2.2 ä»£ç 

4.1.2 èŠ‚ä¸­æ˜ç¡®æŒ‡å‡ºï¼Œå®éªŒç›®æ ‡å˜ä¸ºé™ä½å³°å€¼è´Ÿè·ï¼ˆdistrict-level daily peak loadï¼‰ã€‚æ–‡ä¸­è¯´æ˜ï¼š
â€œConfigurations with peak reduction objectiveâ€¦ The RBC has been fine-tuned to target energy discharge during peak periodsâ€¦â€

è€Œåœ¨ CityLearn ç¯å¢ƒä¸­ï¼ŒKPI ç”± reward_function æ§åˆ¶ã€‚æ‰€ä»¥è¦æŠŠ 4.1.1 çš„â€œcost / emission minimizationâ€ æ”¹æˆ 4.1.2 çš„â€œpeak reductionâ€ï¼Œåªéœ€åœ¨ schema.json é‡Œä¿®æ”¹ reward é…ç½®å³å¯ã€‚

```
"reward_function": {
  "type": "citylearn.reward_function.RewardFunction",
  "attributes": {
    "electricity_cost_weight": 0.0,
    "emission_weight": 0.0,
    "peak_to_average_ratio_weight": 1.0
  }
}

```

å…¶ä»–ä»£ç ä¸ 4.1.1 ç›¸åŒ

## 3. Discomfort and Electricity Consumption Reduction

KPI ä¸ºåœ¨èŠ‚èƒ½çš„åŒæ—¶ç»´æŒèˆ’é€‚åº¦ï¼ˆä¾›æš–ã€çƒ­æ°´æ¸©åº¦ç­‰ï¼‰

## 3.1 ä»£ç 

```
"reward_function": {
  "type": "citylearn.reward_function.ComfortReward",
  "attributes": {
    "band": 1.0,
    "lower_exponent": 2.0,
    "higher_exponent": 3.0,
    "m": 3.0,
    "mode": "cooling"
  }
}
```

## summary

åœ¨ 4.1 éƒ¨åˆ†çš„å®éªŒä¸­ï¼Œé€šè¿‡ä¿®æ”¹ schema.json ä¸­çš„ reward_function é…ç½®å³å¯åˆ‡æ¢ä¸åŒçš„æ€§èƒ½è¯„ä»·æŒ‡æ ‡ï¼ˆKPIï¼‰ä¸æ§åˆ¶ç›®æ ‡ï¼Œä»è€Œå®ç°åŸºäºä¸åŒä¼˜åŒ–ç›®æ ‡çš„ç­–ç•¥è®­ç»ƒ
| ç±»å | æ‰€å±æ¨¡å— | ç®€è¦è¯´æ˜ | å¯é…å‚æ•°ï¼ˆkwargs/attributesï¼‰ç¤ºä¾‹ |
| ------------------------------ | --------------------------- | ------------------------------------------------- | ---------------------------------------------------------------------------------- |
| `RewardFunction` | `citylearn.reward_function` | é»˜è®¤é€šç”¨å¥–åŠ±å‡½æ•°ï¼Œæƒ©ç½šä»ç½‘æ ¼è´­ä¹°ç”µåŠ›ã€‚ ([citylearn.net][1]) | `exponent`ï¼ˆå¦‚ 1.0ï¼‰ã€`charging_constraint_penalty_coefficient` ç­‰ |
| `MARL` | `citylearn.reward_function` | ç”¨äºå¤šä½“ï¼ˆmulti-agentï¼‰åœºæ™¯çš„å¥–åŠ±å‡½æ•° | æ— æˆ–å°‘é‡é¢å¤–å‚æ•° |
| `IndependentSACReward` | `citylearn.reward_function` | æ¨èç”¨äºæ¯å»ºç­‘ç‹¬ç«‹ agent çš„ SAC æ§åˆ¶å™¨å¥–åŠ±ã€‚ | æ— æˆ–å°‘é‡å‚æ•° |
| `SolarPenaltyReward` | `citylearn.reward_function` | é¼“åŠ±å‡€é›¶ï¼æœ€å¤§åŒ–è‡ªå‘ç”µã€å‡å°‘ä»ç”µç½‘è´­ç”µã€‚ | é»˜è®¤è¡Œä¸ºï¼Œæ— éœ€é¢å¤–å‚æ•°æˆ–å¯é…ç½®å‚æ•°å¦‚ç³»æ•° |
| `ComfortReward` | `citylearn.reward_function` | ä»¥å®¤å†…æ¸©åº¦åå·®ï¼ˆèˆ’é€‚åº¦ï¼‰ä¸ºç›®æ ‡çš„å¥–åŠ±å‡½æ•°ã€‚ | `band`ï¼ˆèˆ’é€‚å¸¦å®½ï¼‰ã€`lower_exponent`ã€`higher_exponent` ç­‰ |
| `SolarPenaltyAndComfortReward` | `citylearn.reward_function` | ç»“åˆå¤ªé˜³èƒ½æƒ©ç½š + èˆ’é€‚åº¦æƒ©ç½šçš„æ··åˆå‹å¥–åŠ±ã€‚ | `band`ã€`lower_exponent`ã€`higher_exponent`ã€`coefficients` ç­‰ |

æ‰€æœ‰çš„ reward_function å¯ä»¥åœ¨è¿™çœ‹ï¼šhttps://www.citylearn.net/api/citylearn.reward_function.html?utm_source=chatgpt.com

å½“ç„¶ï¼Œä»¥ä¸Šä»…é’ˆå¯¹ SACï¼Œå¦‚æœæ˜¯ RBCï¼Œéœ€è¦ä¿®æ”¹ rbc.py æ–‡ä»¶é‡Œçš„ç­–ç•¥ï¼Œä¸è¿‡æˆ‘è§‰å¾—é‡ç‚¹æ˜¯ SACï¼Œæ‰€ä»¥æš‚ä¸”å°±å…ˆä¸ç®¡ RBC çš„è§„åˆ™ä¿®æ”¹äº†

# Energy storage system control in representative single-family neighborhoods

å¦‚æœè¯´ 4.1 æ˜¯å¤šç›®æ ‡åŸºå‡†æµ‹è¯•ï¼ˆç¯å¢ƒå›ºå®šï¼Œæ§åˆ¶ç®—æ³•å›ºå®šï¼ŒKPI ä¸åŒï¼‰ï¼Œé‚£ 4.2 å°±æ˜¯**ä¸åŒå»ºç­‘ç±»å‹ï¼ˆå¦‚å•å±‚æˆ¿ã€å¤šå±‚æˆ¿ã€å¸¦å¤ªé˜³èƒ½çš„æˆ¿å±‹ï¼‰å…·æœ‰ä¸åŒè´Ÿè·ç‰¹æ€§ä¸å‚¨èƒ½éœ€æ±‚ï¼ŒåŒæ ·çš„æ§åˆ¶ç®—æ³•åœ¨è¿™äº›â€œä»£è¡¨æ€§ä½å®…â€ä¸Šä¼šè¡¨ç°å‡ºä¸åŒçš„èƒ½è€—åŠ¨æ€ã€‚**

## 1. é‡ç‚¹

1. é‡‡ç”¨ç›¸åŒçš„ RL æ§åˆ¶ç­–ç•¥ï¼ˆé€šå¸¸æ˜¯ SACï¼‰ï¼›
2. åœ¨ä¸åŒçš„ â€œrepresentative neighborhoodsâ€ schema ä¸Šè¿è¡Œï¼›
3. æ¯”è¾ƒç»“æœ
   Â· ç”µæ± å……æ”¾ç”µæ›²çº¿ (SOC)
   Â· è´Ÿè·æ›²çº¿å¹³æ»‘ç¨‹åº¦
   Â· æˆæœ¬æˆ–ç¢³æ’ç»“æœå·®å¼‚

## 2. åŒºåˆ«

| é¡¹ç›®                | 4.1 éƒ¨åˆ†                                                                 | 4.2 éƒ¨åˆ†                                                                |
| ------------------- | ------------------------------------------------------------------------ | ----------------------------------------------------------------------- |
| **å®éªŒä¸»é¢˜**        | Benchmark å®éªŒï¼šåœ¨ç»Ÿä¸€åŸå¸‚ç¤¾åŒºä¸‹æµ‹è¯•ä¸åŒ KPIï¼ˆæˆæœ¬ã€ç¢³æ’ã€å³°å€¼ã€èˆ’é€‚åº¦ï¼‰ | åº”ç”¨å®éªŒï¼šæµ‹è¯•æ§åˆ¶ç®—æ³•åœ¨**ä»£è¡¨æ€§ä½å®…ç¤¾åŒº**çš„çœŸå®å‚¨èƒ½æ§åˆ¶æ€§èƒ½            |
| **å®éªŒç›®æ ‡ï¼ˆKPIï¼‰** | åˆ‡æ¢ä¸åŒ reward_functionï¼Œæ¯”è¾ƒä¸åŒä¼˜åŒ–ç›®æ ‡                               | å›ºå®š rewardï¼ˆé€šå¸¸æ˜¯ cost/emissionï¼‰ï¼Œè€ƒå¯Ÿç®—æ³•åœ¨ä¸åŒå»ºç­‘ç±»å‹ä¸‹çš„è¡Œä¸ºå·®å¼‚ |
| **æ§åˆ¶ç»“æ„**        | é›†ä¸­å¼æˆ–åŠé›†ä¸­å¼æ§åˆ¶ï¼ˆcentral_agent å¯ Trueï¼‰                            | **åˆ†å¸ƒå¼æ§åˆ¶**ï¼ˆcentral_agent=Falseï¼Œæ¯æ ‹å»ºç­‘ç‹¬ç«‹ agentï¼‰               |
| **ç¯å¢ƒè§„æ¨¡**        | é€šå¸¸æ˜¯å®Œæ•´åŸå¸‚æˆ–åŒºåŸŸï¼ˆå¤šä¸ªä½å®…+å•†ç”¨å»ºç­‘ï¼‰                                | ç¼©å°åˆ°å•å®¶åº­æˆ–å°‘é‡ä»£è¡¨æ€§ä½å®…ç¾¤                                          |
| **é‡ç‚¹åˆ†æ**        | â€œä¸åŒ KPI ä¸‹çš„ä¼˜åŒ–ç›®æ ‡â€                                                  | â€œå‚¨èƒ½ç³»ç»Ÿåœ¨ç°å®ä½å®…åœºæ™¯ä¸­çš„åŠ¨æ€æ§åˆ¶è¡¨ç°â€                                |
| **è¾“å‡ºæŒ‡æ ‡**        | reward æ›²çº¿ã€æ€»èƒ½è€—ã€PAR                                                 | æ¯æˆ·å»ºç­‘çš„ç”µæ±  SOC æ›²çº¿ã€åŠŸç‡è´Ÿè·ã€å³°å€¼å˜åŒ–è¶‹åŠ¿                         |

## 3. ä»£ç 

å’Œ 4.1 å…¶å®å¤§åŒå°å¼‚ï¼Œcentral_agent=False å³å¯ï¼Œçªå‡ºå„ agent å•ç‹¬è®­ç»ƒ

```
# ============================================================
# CityLearn 4.2 å®éªŒå¤ç°: Energy storage system control
# å‚è€ƒè®ºæ–‡ Section 4.2
# ============================================================

from citylearn.citylearn import CityLearnEnv
from citylearn.agents.base import BaselineAgent
from citylearn.agents.rbc import BasicRBC
from citylearn.agents.sac import SAC
from pathlib import Path
from debug_sac import DebugSAC  # âœ… ä½¿ç”¨è°ƒè¯•ç‰ˆæœ¬ï¼Œä¸æ”¹åº“



# æµ‹è¯•ç”¨ï¼Œä»…ä½¿ç”¨næ ‹buildings
def run(agent_class, schema, episodes=1, central_agent=False, save_dir=None):
    print(f"\n=== Running {agent_class.__name__} | Central Agent = {central_agent} ===")

    # âœ… æ­£ç¡®é€‰æ‹©å‰ä¸‰æ ‹å»ºç­‘
    env_full = CityLearnEnv(schema, central_agent=central_agent)
    selected_buildings = env_full.buildings[:20]
    env_full.close()

    # âœ… ç”¨3æ ‹å»ºç­‘åˆ›å»ºæ–°ç¯å¢ƒ
    env = CityLearnEnv(schema, buildings=selected_buildings, central_agent=central_agent)
    env.reset()

    agent = agent_class(env)
    agent.learn(episodes=episodes, deterministic_finish=True)

    results = env.evaluate()
    env.close()

    print(f"âœ… {agent_class.__name__} finished.")
    print(f"ğŸ“Š Results: {results}")

    if save_dir:
        Path(save_dir).mkdir(parents=True, exist_ok=True)
        (Path(save_dir) / "metrics.txt").write_text(str(results))

    return results


# def run(agent_class, schema, episodes=1, central_agent=False, save_dir=None):
#     print(f"\n=== Running {agent_class.__name__} | Central Agent = {central_agent} ===")

#     # âœ… åˆ›å»ºç¯å¢ƒ
#     env = CityLearnEnv(schema, buildings=env.buildings[:3], central_agent=False)
#     # env = CityLearnEnv(schema, central_agent=central_agent)
#     env.reset()

#     # âœ… åˆ›å»º Agent
#     agent = agent_class(env)

#     # âœ… è®­ç»ƒ Agentï¼ˆBaseline / RBC ä¼šè‡ªåŠ¨è·³è¿‡ learnï¼‰
#     agent.learn(episodes=episodes, deterministic_finish=True)

#     # âœ… è¯„ä¼°æŒ‡æ ‡
#     results = env.evaluate()
#     env.close()

#     print(f"âœ… {agent_class.__name__} finished.")
#     print(f"ğŸ“Š Results: {results}")

#     # âœ… ä¿å­˜ç»“æœ
#     if save_dir:
#         Path(save_dir).mkdir(parents=True, exist_ok=True)
#         (Path(save_dir) / "metrics.txt").write_text(str(results))

#     return results

def main():
    schema = "data/datasets/ca_alameda_county_neighborhood/schema.json"
    central_agent = False

    # Baseline
    run(
        agent_class=BaselineAgent,
        schema=schema,
        episodes=1,
        central_agent=central_agent,
        save_dir="results/4_2_baseline_CA"
    )

    # RBC
    run(
        agent_class=BasicRBC,
        schema=schema,
        episodes=1,
        central_agent=central_agent,
        save_dir="results/4_2_rbc_CA"
    )

    # SAC
    run(
        agent_class=DebugSAC,
        schema=schema,
        episodes=5,
        central_agent=central_agent,
        save_dir="results/4_2_sac_CA"
    )

if __name__ == "__main__":
    main()

```

## 4. æŠ¥é”™

### 4.1 AssertionError

```
AssertionError: demand is greater than heating_device max output | timestep: 0, building: resstock-amy2018-2021-release-1-122940, outage: False, demand: 3.1614, output: 2.9663, difference: 0.1950
```

CityLearn åœ¨ building.py é‡Œå†™äº†å®‰å…¨æ£€æŸ¥,ç±»å‹å¦‚ä¸‹
| åŸå› ç±»å‹ | è¯´æ˜ | æ˜¯å¦è‡´å‘½ |
| ---------------- | ---------------------------------------------------- | ----------- |
| **1ï¸âƒ£ æ•°æ®å¼‚å¸¸** | æŸä¸ªä½å®…åœ¨è¾“å…¥æ•°æ®ï¼ˆæ¸©åº¦ / è´Ÿè·ï¼‰é¦–å°æ—¶æ•°å€¼å¼‚å¸¸ï¼Œä½¿ heating_demand è¶…å‡ºè®¾å¤‡é¢å®šåŠŸç‡ | âŒ å¯è·³è¿‡ |
| **2ï¸âƒ£ æ¨¡å‹åˆå§‹åŒ–è¯¯å·®** | åœ¨ simulation_start_time_step=0 æ—¶ï¼Œç³»ç»ŸçŠ¶æ€è¿˜æ²¡åˆå§‹åŒ–å®Œå…¨ï¼Œå‡ºç°è½»å¾®ä¸ä¸€è‡´ | âŒ å¯å¿½ç•¥ |
| **3ï¸âƒ£ æ¸©æ§æ¨¡å‹ç²¾åº¦é—®é¢˜** | HVAC æ¨¡å‹åœ¨æŸäº›æ—¶åˆ»è®¡ç®—çš„ heat_demand ä¸è®¾å¤‡å‚æ•°æœ‰å¾®å°è¯¯å·® | âŒ å¯é€šè¿‡å®¹å·®å‚æ•°æ”¾å®½ |

å¯ä»¥æ‰“å¼€ citylearn/building.py
æŸ¥æ‰¾å¹¶ä¿®æ”¹ï¼š

```
# before
assert self.power_outage or demand <= max_device_output or abs(demand - max_device_output) < TOLERANCE, \
    f"demand is greater than heating_device max output | ..."

# now
if not (self.power_outage or demand <= max_device_output or abs(demand - max_device_output) < TOLERANCE):
    print(f"[Warning] demand > heating_device max output at timestep {self.time_step}")
```

æˆ–è€…è·‘ä»£ç çš„æ—¶å€™ç›´æ¥ python -O run_4_2.pyï¼ˆå¿½ç•¥æ‰€æœ‰ assert è¯­å¥ï¼‰

æˆ–è€…æŠŠå®¹å¿åº¦ä» 1e-4 æ”¹å¤§ä¸€äº›ä¹Ÿè¡Œ

```
TOLERANCE = 1e-4
```

ä½†æ˜¯æ€»ä½“è€Œè¨€é—®é¢˜ä¸å¤§ï¼Œè®ºæ–‡é‡Œè¯´äº†ï¼Œåœ¨åŸºäº ResStock çš„å»ºç­‘æ•°æ®ä¸­ï¼Œç”±äºç¼©æ”¾å·®å¼‚ï¼Œåˆå§‹åŒ–æ—¶å¯èƒ½ä¼šå‡ºç°è½»å¾®çš„è®¾å¤‡å®¹é‡è¶…é™ï¼Œè¿™ä¸ä¼šå½±å“å®éªŒç»“æœã€‚

### 4.2 TypeError: new(): data must be a sequence (got NoneType)

å…ˆçœ‹æŠ¥é”™å†…å®¹

```
(citylearnv2) root@autodl-container-491e4b9a34-9e5b11d5:~/CityLearn# python -O run_4_2.py
Couldn't import dot_parser, loading of dot files will not be possible.

=== Running SAC ===
Traceback (most recent call last):
  File "/root/CityLearn/run_4_2.py", line 44, in <module>
    main()
  File "/root/CityLearn/run_4_2.py", line 40, in main
    run(SAC, schema, episodes=5, central_agent=central_agent,
  File "/root/CityLearn/run_4_2.py", line 16, in run
    agent.learn(episodes=episodes, deterministic_finish=True)
  File "/root/CityLearn/citylearn/agents/base.py", line 156, in learn
    actions = self.predict(observations, deterministic=deterministic)
  File "/root/CityLearn/citylearn/agents/sac.py", line 189, in predict
    actions = self.get_post_exploration_prediction(observations, deterministic)
  File "/root/CityLearn/citylearn/agents/sac.py", line 206, in get_post_exploration_prediction
    o = torch.FloatTensor(o).unsqueeze(0).to(self.device)
TypeError: new(): data must be a sequence (got NoneType)
```

å¸¸è§æˆå› å¦‚ä¸‹

1. åˆå§‹åŒ–è§‚æµ‹æ²¡å‡†å¤‡å¥½
   éƒ¨åˆ†ç‰ˆæœ¬/æ•°æ®ï¼ˆå°¤å…¶ LSTM/ResStock å»ºç­‘ï¼‰åœ¨åˆ›å»ºåç¬¬ä¸€æ‹çš„è§‚æµ‹å¯èƒ½ä¸ºç©ºï¼š
   Â· æ²¡ reset()
   Â· æˆ– reset() è¿”å›å®Œç¬¬ä¸€å¸§ä»éœ€è¦ä¸€ä¸ªâ€œçƒ­èº« stepâ€æ‰æœ‰æœ‰æ•ˆ obs

2. schema çš„è§‚æµ‹è¢«å…³äº†
   å¦‚æœ root.observations é‡Œå…¨éƒ½ "active": falseï¼Œæˆ–æŸäº›å»ºç­‘åœ¨ buildings.\*.inactive_observations é‡ŒæŠŠæ‰€æœ‰å¯è§‚æµ‹é‡éƒ½ç¦ç”¨äº†ï¼Œè¯¥å»ºç­‘ï¼ˆç”šè‡³å…¨å±€ï¼‰è§‚æµ‹ä¼šå˜æˆ None/ç©ºã€‚

3. è§‚æµ‹å½¢æ€ä¸åŒ¹é…
   central_agent=True â†’ æœŸæœ›å¾—åˆ°â€œä¸€ä¸ªè”åˆå‘é‡â€ï¼›
   central_agent=False â†’ æœŸæœ›â€œæ¯æ ‹ä¸€ä¸ªå‘é‡çš„åˆ—è¡¨â€ã€‚
   å¦‚æœæœ‰å»ºç­‘ include: false / è§‚æµ‹è¢«è£ï¼Œåˆ—è¡¨é‡Œå°±ä¼šæ··å…¥ Noneã€‚

**PSï¼šå¤ªä»–å¦ˆå˜æ€äº†è¿™ä¸ªé—®é¢˜ï¼Œè¿™ä¸ª error å¡äº†æˆ‘å°†è¿‘ä¸€å‘¨æ‰çŸ¥é“å“ªé‡Œå‡ºç°äº†é—®é¢˜**

è§£å†³æ–¹æ³•ï¼ˆè¿‡äºå˜æ€ï¼Œæˆ‘é€‰æ‹©è®²æˆ‘æ€ä¹ˆè§£å†³çš„ï¼‰

1. ä»æ•°æ®é›†å…¥æ‰‹
   å¯ä»¥è¿è¡Œä¸€ä¸‹è¿™ä¸ªä»£ç ï¼Œå…ˆæŸ¥çœ‹ building å’Œ obs é‡Œæœ‰æ²¡æœ‰ none

```
# inspect_schema.py
import json, sys, argparse, os
from typing import Any, Dict, List

def load_schema(path: str) -> Dict[str, Any]:
    txt = open(path, 'r', encoding='utf-8').read()
    # å…ˆå°è¯• JSON
    try:
        data = json.loads(txt)
    except Exception:
        # å¯é€‰ï¼šYAMLï¼ˆè‹¥å®‰è£…äº† PyYAMLï¼‰
        try:
            import yaml  # pip install pyyaml
            data = yaml.safe_load(txt)
        except Exception as e:
            print(f"âŒ æ—¢ä¸æ˜¯ JSON ä¹Ÿä¸æ˜¯å¯è§£æçš„ YAMLï¼š{e}")
            sys.exit(1)
    # å¤§å¤šæ•° CityLearn schema é¡¶å±‚æœ‰ä¸ª 'root'
    return data.get('root', data)

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument('--schema', required=True, help='path/to/schema.json')
    args = ap.parse_args()

    sc = load_schema(args.schema)

    print("\n=== Top-level keys ===")
    print(list(sc.keys()))

    # 1) central_agent & sim period
    print("\n=== Simulation/Agent ===")
    print("central_agent:", sc.get('central_agent'))
    print("simulation_start_time_step:", sc.get('simulation_start_time_step'))
    print("simulation_end_time_step:", sc.get('simulation_end_time_step'))
    print("seconds_per_time_step:", sc.get('seconds_per_time_step'))

    # 2) observationsï¼ˆå…¨å±€ï¼‰
    obs = sc.get('observations', {})
    print("\n=== Global Observations ===")
    if not obs:
        print("âš ï¸  observations æ˜¯ç©ºçš„ï¼ˆè¿™ä¼šå¯¼è‡´ SAC è§‚æµ‹ä¸º Noneï¼‰")
    else:
        actives = [k for k,v in obs.items() if isinstance(v, dict) and v.get('active', False)]
        inactives = [k for k,v in obs.items() if isinstance(v, dict) and not v.get('active', False)]
        print("active æ•°é‡:", len(actives))
        print("active åˆ—è¡¨(å‰20):", actives[:20])
        if inactives:
            print("inactive åˆ—è¡¨(å‰20):", inactives[:20])

    # 3) buildingsï¼ˆå­—å…¸æˆ–åˆ—è¡¨ï¼‰
    b = sc.get('buildings', {})
    print("\n=== Buildings ===")
    if isinstance(b, dict):
        b_names = list(b.keys())
        print("å»ºç­‘æ•°é‡:", len(b_names))
        sample = b_names[:5]
        print("ç¤ºä¾‹å»ºç­‘:", sample)
        # æ£€æŸ¥ include / inactive_observations
        problem_buildings: List[str] = []
        fully_inactive: List[str] = []
        for name in b_names:
            cfg = b[name] or {}
            include = cfg.get('include', True)
            if not include:
                problem_buildings.append(name)
            inact = cfg.get('inactive_observations', [])
            # å¦‚æœæŠŠå…¨å±€ active éƒ½å±è”½äº†ï¼Œä¹Ÿä¼šå¯¼è‡´è¯¥å»ºç­‘ obs=None
            try:
                actives = [k for k,v in obs.items() if isinstance(v, dict) and v.get('active', False)]
                if actives and isinstance(inact, list) and set(inact) >= set(actives):
                    fully_inactive.append(name)
            except Exception:
                pass
        if problem_buildings:
            print(f"âš ï¸  æœ‰ {len(problem_buildings)} æ ‹å»ºç­‘ include=falseï¼ˆå¯èƒ½å¯¼è‡´ Noneï¼‰ï¼š", problem_buildings[:5])
        if fully_inactive:
            print(f"âš ï¸  æœ‰ {len(fully_inactive)} æ ‹å»ºç­‘æŠŠæ‰€æœ‰å…¨å±€è§‚æµ‹éƒ½å±è”½äº†ï¼š", fully_inactive[:5])
    else:
        print("âš ï¸  buildings ä¸æ˜¯å­—å…¸ï¼ˆè¯·è´´å†…å®¹æˆ‘å†çœ‹ï¼‰")

    # 4) å¿«é€Ÿâ€œç¯å¢ƒæ¢é’ˆâ€ï¼ˆä¸è®­ç»ƒï¼Œåª reset çœ‹çœ‹ obs å½¢æ€ï¼‰
    try:
        from citylearn.citylearn import CityLearnEnv
        ca = sc.get('central_agent', False)
        print("\n=== Env probe (reset) ===")
        env = CityLearnEnv(args.schema, central_agent=ca)
        obs0 = env.reset()
        print("reset() è¿”å›ç±»å‹:", type(obs0))
        if obs0 is None:
            print("âŒ reset() è¿”å› None â€”â€” SAC ä¼šåœ¨ FloatTensor(None) å¤„æŠ¥é”™ã€‚")
        else:
            try:
                # centralized â†’ å‘é‡/æ•°ç»„ï¼›decentralized â†’ list[æ•°ç»„]
                if isinstance(obs0, list):
                    shapes = [getattr(x, 'shape', None) for x in obs0]
                    print("åˆ†å¸ƒå¼è§‚æµ‹æ•°é‡:", len(obs0), "ç¤ºä¾‹å½¢çŠ¶:", shapes[:5])
                else:
                    print("é›†ä¸­å¼è§‚æµ‹å½¢çŠ¶:", getattr(obs0, 'shape', None))
            except Exception:
                pass
        env.close()
    except Exception as e:
        print("ï¼ˆå¯é€‰æ¢é’ˆå¤±è´¥ï¼šï¼‰", e)

if __name__ == '__main__':
    main()
```

![](http://8.130.141.48/wp-content/uploads/2025/11/17622520053493-scaled.jpg)
æˆ‘çš„è¿è¡Œç»“æœ

1. è¿›è¡Œè°ƒè¯•
   ä»æŠ¥é”™ä¸­å¯ä»¥çŸ¥é“ï¼Œé—®é¢˜ä¸»è¦æ˜¯å‡ºç°åœ¨ sac.py çš„ self.get_post_exploration_prediction é‡Œï¼Œå†å»ºä¸€ä¸ª py æ–‡ä»¶é‡å†™è¯¥ç±»ï¼ŒæŸ¥çœ‹åˆ°åº•æ˜¯å“ªä¸€æ­¥å‡ºç°äº†é—®é¢˜

```
import numpy as np
import torch
from citylearn.agents.sac import SAC

class DebugSAC(SAC):
    """è°ƒè¯•ç”¨ SACï¼šä¸æ”¹åŸåº“ï¼Œä½†æ‰“å° observation & None æ¥æº"""

    def get_post_exploration_prediction(self, observations, deterministic=False):

        # âœ… å¦‚æœæ˜¯ dictï¼Œå…ˆè½¬æ¢ä¸º listï¼Œé¿å…åŸå§‹é”™è¯¯
        if isinstance(observations, dict):
            observations = [obs for obs in observations.values()]

        actions = []

        for i, o in enumerate(observations):
            # âœ… Debugï¼šæ‰“å°åŸå§‹ obs
            print(f"\nğŸ” [Debug] Building {i} raw obs = {o}")

            if o is None:
                raise ValueError(f"âŒ Observation from agent {i} is None (raw environment output)")

            # âœ… ç¼–ç 
            encoded = self.get_encoded_observations(i, o)
            print(f"   â¤ Encoded obs = {encoded}")

            if encoded is None:
                raise ValueError(f"âŒ get_encoded_observations() returned None for agent {i}")

            # âœ… å½’ä¸€åŒ–
            normalized = self.get_normalized_observations(i, encoded)
            print(f"   â¤ Normalized obs = {normalized}")

            if normalized is None:
                raise ValueError(f"âŒ get_normalized_observations() returned None for agent {i}")

            # âœ… å°è¯•è½¬æˆ tensor
            try:
                o_tensor = torch.FloatTensor(normalized).unsqueeze(0).to(self.device)
            except Exception as e:
                print(f"âŒ Failed to convert obs to tensor. Obs = {normalized}")
                raise e

            # âœ… åŸæ¥çš„ action è·å–æµç¨‹
            result = self.policy_net[i].sample(o_tensor)
            a = result[2] if deterministic else result[0]
            actions.append(a.detach().cpu().numpy()[0])

        return actions
```

æŠŠæ­¥éª¤åˆ†å¼€ï¼Œç»ˆäºï¼æ‰¾åˆ°äº†é—®é¢˜æ‰€åœ¨--normalizedï¼Œä¸ºä»€ä¹ˆä¼šæœ‰è¿™ä¸ªé—®é¢˜å‘¢ï¼Ÿ
**reasonï¼š**
CityLearn SAC ä¸­å½’ä¸€åŒ–æœºåˆ¶æ²¡æœ‰è¢«åˆå§‹åŒ– / æœªå¯ç”¨æˆ–é‡åˆ°äº†æ— æ•ˆå€¼ï¼Œæ‰€ä»¥ get_normalized_observations() è¿”å›äº† None â†’ ä¼ ç»™ torch.FloatTensor() â†’ æŠ¥é”™
è¿™æ˜¯ CityLearn SAC æ¡†æ¶çš„ä¸€ä¸ªâ€œè®¾è®¡ç¼ºé™·â€ï¼ˆé»˜è®¤ä¸å¼€å¯ normalizationï¼Œåˆæ²¡æœ‰ fallback æœºåˆ¶ï¼‰
citylearn çš„é»˜è®¤æµç¨‹æ˜¯è¿™æ ·çš„ï¼š

1. normalization_enable é»˜è®¤æ˜¯ True
2. ä½† normalization å‚æ•°ï¼ˆmean/stdï¼‰åªä¼šåœ¨è®­ç»ƒè‹¥å¹²æ­¥åæ‰æ›´æ–°
3. ç¬¬ 1 ä¸ª timestep ç›´æ¥è°ƒç”¨ predict() â†’ normalization çŸ©é˜µè¿˜æ²¡æœ‰ â†’ è¿”å› None

**soluation**
å½’ä¸€åŒ–åœ¨ SAC ä¸­è¿˜æ˜¯éœ€è¦çš„ï¼Œä¸ç„¶ä¼šéš¾ä»¥æ”¶æ•›æˆ–ä¸ç¨³å®šï¼Œè€Œä¸”ä¸èƒ½ä¿®æ”¹ schema é‡Œçš„æ•°æ®ï¼ˆjson é‡Œä¿å­˜çš„éƒ½æ˜¯çœŸå®åŸå§‹æ•°æ®ï¼‰
ä¸¤ç§æ–¹æ³•

1. åœ¨è®­ç»ƒå‰è‡ªåŠ¨æ”¶é›†æ•°æ®ï¼Œåˆå§‹åŒ– mean/std

```
# åˆå§‹åŒ– normalization
obs_buffer = []

obs = env.reset()
for _ in range(500):  # æ”¶é›†å‰ 500 æ­¥
    action = env.action_space.sample()   # éšæœºåŠ¨ä½œ
    obs, _, _, _ = env.step(action)
    obs_list = list(obs.values()) if isinstance(obs, dict) else obs
    obs_buffer.extend(obs_list)

obs_buffer = np.array(obs_buffer)
agent.obs_mean = np.mean(obs_buffer, axis=0)
agent.obs_std = np.std(obs_buffer, axis=0) + 1e-6  # é¿å…é™¤0

print("âœ… Normalization initialized.")
```

2. ä¸­é€”åŠ¨æ€å½’ä¸€åŒ–
   åœ¨ SAC å†…éƒ¨æ¯æ”¶é›†ä¸€æ‰¹æ•°æ®å°±æ›´æ–° mean/stdï¼ˆç±»ä¼¼ running meanï¼‰,åªéœ€è®­ç»ƒå‰æ‰‹åŠ¨è¿è¡Œå‡ æ­¥ env.step() + agent.observe() å³å¯ã€‚

## summary

4.1 æ˜¯ç®—æ³•åœ¨ä¸åŒ KPI ä¸‹çš„â€œç›®æ ‡å‡½æ•°å¯¹æ¯”å®éªŒâ€ï¼›
4.2 æ˜¯åŒä¸€ç®—æ³•åœ¨ä¸åŒä½å®…åœºæ™¯ä¸‹çš„â€œå®é™…å‚¨èƒ½æ§åˆ¶å®éªŒâ€ã€‚
é’ˆå¯¹ TypeError: new():ï¼š
åœ¨ CityLearn + SAC ä¸­ï¼Œå½’ä¸€åŒ–ä¹‹æ‰€ä»¥ä¼šå‡ºç° None å¹¶æŠ¥é”™ï¼Œæ˜¯å› ä¸ºå¼ºåŒ–å­¦ä¹ é‡Œçš„æ™ºèƒ½ä½“ï¼ˆagentï¼‰æ˜¯é€šè¿‡ä¸ç¯å¢ƒé€æ­¥äº¤äº’æ¥â€œå­¦ä¹ è§‚å¯Ÿæ•°æ®çš„åˆ†å¸ƒâ€ï¼Œè€Œä¸æ˜¯åƒä¼ ç»Ÿæœºå™¨å­¦ä¹ é‚£æ ·åœ¨ä¸€å¼€å§‹å°±åŠ è½½æ‰€æœ‰æ•°æ®å¹¶è®¡ç®—æ•´ä½“å‡å€¼å’Œæ ‡å‡†å·®ã€‚å½’ä¸€åŒ–éœ€è¦ç”¨åˆ°è§‚æµ‹æ•°æ®çš„å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆmean/stdï¼‰ï¼Œä½†åœ¨è®­ç»ƒåˆæœŸï¼Œagent è¿˜æ²¡æœ‰æ”¶é›†åˆ°è¶³å¤Ÿçš„è§‚æµ‹æ•°æ®ï¼Œè¿™äº›å½’ä¸€åŒ–å‚æ•°å°šæœªå»ºç«‹ï¼Œå› æ­¤ get_normalized_observations() è¿”å›äº† Noneã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œé”™è¯¯å¹¶ä¸æ˜¯ç”±åŸå§‹æ•°æ®æˆ–ç¯å¢ƒé€ æˆçš„ï¼Œè€Œæ˜¯å› ä¸ºâ€œagent è¿˜ä¸äº†è§£ç¯å¢ƒçš„æ•°æ®åˆ†å¸ƒï¼Œå´æå‰å°è¯•å¯¹æ•´ä¸ªç¯å¢ƒè¿›è¡Œå½’ä¸€åŒ–â€ã€‚è§£å†³æ–¹æ³•å¯ä»¥æ˜¯è·³è¿‡å½’ä¸€åŒ–ã€åœ¨è®­ç»ƒå‰è¿›è¡Œ warm-up æ”¶é›†æ•°æ®ä»¥åˆå§‹åŒ–å‡å€¼å’Œæ ‡å‡†å·®ï¼Œæˆ–ä½¿ç”¨ centralized agent ç­‰æ–¹å¼ã€‚

# Vehicle-to-Grid Control

## 1. é‡ç‚¹ä¸ç›®æ ‡

åœ¨ CityLearn ç¯å¢ƒä¸­æ‰©å±•å¼ºåŒ–å­¦ä¹ æ¨¡å‹ï¼Œä½¿å¾—ç”µåŠ¨æ±½è½¦ä¸ä»…èƒ½å……ç”µï¼Œè¿˜èƒ½åœ¨é«˜å³°æ—¶æ®µå°†ç”µèƒ½å›é¦ˆç»™å»ºç­‘æˆ–ç”µç½‘ï¼Œä»¥å®ç°å‰Šå³°å¡«è°·å’Œæ•´ä½“èƒ½è€—ä¼˜åŒ–ï¼Œç›®æ ‡å¦‚ä¸‹

1. å°† V2G æœºåˆ¶å¼•å…¥æ™ºèƒ½ç”µç½‘ä¸­çš„å¤šå»ºç­‘æ§åˆ¶æ¡†æ¶ï¼›
2. æ¢ç©¶ RL æ™ºèƒ½ä½“å¦‚ä½•åœ¨å»ºç­‘å‚¨èƒ½ç³»ç»Ÿä¸ç”µåŠ¨æ±½è½¦ç”µæ± ä¹‹é—´åè°ƒèƒ½é‡æµåŠ¨ï¼›
3. ç›®æ ‡å‡½æ•°ä¸ä»…è€ƒè™‘ç”µè´¹æœ€å°åŒ–ï¼Œè¿˜è€ƒè™‘æ’æ”¾ã€å³°å€¼è´Ÿè·å’Œèˆ’é€‚åº¦çº¦æŸã€‚

## 2. ç¯å¢ƒè®¾ç½®

åœ¨ CityLearn ç¯å¢ƒä¸­ï¼Œæ¯æ ‹å»ºç­‘ä¸ä»…æœ‰ battery å‚¨èƒ½ç³»ç»Ÿï¼Œè¿˜é¢å¤–é…ç½® EV batteryï¼ŒRL æ™ºèƒ½ä½“çš„åŠ¨ä½œç©ºé—´åŒ…æ‹¬ï¼š

```math
a_t = \left[ a_{\text{building battery}},\ a_{\text{EV battery}} \right]
```

å…¶ä¸­æ¯ä¸ª`a_t`éƒ½è¡¨ç¤ºå……/æ”¾ç”µæ¯”ä¾‹ï¼›

- EV çš„**å¯ç”¨æ—¶é—´çª—å£**ç”±ä½å®…ç”¨æˆ·è¡Œä¸ºæ•°æ®ç¡®å®šï¼ˆè½¦è¾†é€šå¸¸ç™½å¤©ç¦»å®¶ã€å¤œæ™šæ¥å…¥ç”µç½‘ï¼‰ï¼›
- å……ç”µåŠŸç‡å—é™äº`P_{\max}`ï¼ŒåŒæ—¶éµå®ˆ SoC ä¸Šä¸‹ç•Œçº¦æŸï¼›
- åŠ¨ä½œç»è¿‡å½’ä¸€åŒ–ä¸è£å‰ªï¼Œä¿è¯ç‰©ç†å¯è¡Œæ€§ã€‚

## 3. å¥–åŠ±å‡½æ•°

å…¸å‹å½¢å¼ä¸ºï¼š

```math
r_t = -\alpha C_t - \beta E_t - \gamma P_{\text{peak},t}
```

where

- `C_t`ï¼šç”¨ç”µæˆæœ¬ï¼›
- `E_t`ï¼šç¢³æ’æ”¾ï¼›
- `P_{\text{peak},t}`ï¼šå³°å€¼åŠŸç‡ï¼›
- `\alpha, \beta, \gamma`ï¼šæƒé‡ç³»æ•°ã€‚
  é€šè¿‡ V2Gï¼Œæ™ºèƒ½ä½“å­¦ä¼šåœ¨é«˜ä»·æ—¶æ®µæ”¾ç”µã€ä½ä»·æ—¶æ®µå……ç”µï¼Œä»è€Œé™ä½ç»¼åˆæˆæœ¬ã€‚

## 4. åŒºåˆ«

| å¯¹æ¯”é¡¹             | **4.1 Baseline Control (RBC/Baseline)** | **4.2 Reinforcement Learning Control (SAC)** | **4.3 Vehicle-to-Grid Control (V2G)**         |
| :----------------- | :-------------------------------------- | :------------------------------------------- | :-------------------------------------------- |
| **æ§åˆ¶å¯¹è±¡**       | ä»…å»ºç­‘ç”µæ±  (Building Battery)           | åŒä¸Š                                         | å»ºç­‘ç”µæ±  + ç”µåŠ¨è½¦ç”µæ±  (Building + EV Battery) |
| **schema æ–‡ä»¶**    | `schema.json`ï¼ˆåŸºç¡€å»ºç­‘ï¼‰               | `schema.json`ï¼ˆç›¸åŒï¼‰                        | `v2g_schema.json` æˆ– `include_ev=True`        |
| **åŠ¨ä½œç©ºé—´**       | $a_t = [a_{building}]$                  | åŒä¸Š                                         | $a_t = [a_{building}, a_{EV}]$                |
| **çŠ¶æ€ç©ºé—´**       | å»ºç­‘ç”µè´Ÿè·ã€å¤ªé˜³èƒ½ã€SoCã€ç”µä»·ç­‰         | åŒä¸Š                                         | é¢å¤–åŒ…å« EV SoCã€EV å¯ç”¨æ—¶é—´çª—å£              |
| **å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“** | æ— ï¼ˆè§„åˆ™æ§åˆ¶ï¼‰                          | SAC / PPO                                    | SACï¼ˆæ”¯æŒ EV å……æ”¾ç”µè°ƒåº¦ï¼‰                     |
| **ç›®æ ‡å‡½æ•°**       | æœ€å°åŒ–ç”µè´¹                              | åŒä¸Š                                         | æœ€å°åŒ–ç”µè´¹ + æ’æ”¾ + å³°å€¼åŠŸç‡ï¼ˆå¤šç›®æ ‡ï¼‰        |
| **æ–°å‚æ•°**         | â€”                                       | â€”                                            | `v2g=True` æˆ– `include_ev=True`               |
| **ä»¿çœŸç‰¹ç‚¹**       | å•èƒ½ç³»ç»Ÿ                                | å•èƒ½ç³»ç»Ÿ + RL æ§åˆ¶                           | å¤šèƒ½ç³»ç»Ÿï¼ˆå»ºç­‘ + è½¦è¾† + ç”µç½‘ï¼‰                |
| **åˆ›æ–°ç‚¹**         | ä¼ ç»Ÿæ§åˆ¶åŸºçº¿                            | RL é€‚åº”æ€§å­¦ä¹                                 | RL æ‰©å±•åˆ°åŒå‚¨èƒ½ååŒä¼˜åŒ–                       |

ä¸€å¥è¯æ¦‚æ‹¬å°±æ˜¯ï¼Œ4.3 åœ¨ RL åŸºç¡€ä¸Šè¿›ä¸€æ­¥æ‰©å±•åˆ°â€œå»ºç­‘ + ç”µåŠ¨è½¦â€åŒå‚¨èƒ½æ§åˆ¶ï¼Œå³ Vehicle-to-Grid (V2G)

## 5. ä»£ç 

ä»¥ 4.1 çš„ SAC éƒ¨åˆ†ä¸º baselineï¼Œå¦‚ä¸‹

```
from citylearn.citylearn import CityLearnEnv
from citylearn.agents.sac import SAC
from pathlib import Path

def run_v2g(agent_class, schema, episodes=3, central_agent=False, save_dir=None, **kwargs):
    print(f"\n=== Running {agent_class.__name__} (Vehicle-to-Grid) ===")
    # âœ… å¯ç”¨ include_evs / v2g å‚æ•°
    env = CityLearnEnv(schema, central_agent=central_agent, include_evs=True, **kwargs)

    agent = agent_class(env)
    agent.learn(episodes=episodes, deterministic_finish=True)
    results = env.evaluate()
    env.close()

    print(f"âœ… {agent_class.__name__} (V2G) finished.")
    if save_dir:
        Path(save_dir).mkdir(parents=True, exist_ok=True)
        (Path(save_dir)/"metrics.txt").write_text(str(results))
    return results

def main():
    # âœ… ä½¿ç”¨å« EV battery çš„ V2G ç‰ˆæœ¬ schema
    schema_v2g = "data/citylearn_challenge_2022/v2g_schema.json"

    # âœ… è¿è¡Œ SAC æ§åˆ¶å™¨ï¼ˆVehicle-to-Grid å®éªŒï¼‰
    res_v2g = run_v2g(SAC, schema_v2g, episodes=5, central_agent=False, save_dir="results/4_3_v2g_sac")

    print("\n=== V2G SAC Results ===")
    print(res_v2g)

if __name__ == "__main__":
    main()
```

# Occupant comfort feedback during automated demand response

è²Œä¼¼å¤ç°ä¸å‡ºæ¥ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªåŸºäº CityLearn çš„ EULP æ•°æ® + Ecobee DYD çš„çœŸå®ç”¨æˆ·ç»Ÿè®¡æ¨¡å‹ï¼Œ
æ„å»ºå‡ºçš„åŠå®æ•°æ®ä»¿çœŸæ¡ˆä¾‹ï¼ˆsemi-synthetic simulationï¼‰ã€‚

## 1. ç›®çš„

ç ”ç©¶åœ¨è‡ªåŠ¨åŒ–éœ€é‡å“åº”ï¼ˆAutomated Demand Response, ADRï¼‰æœŸé—´å¼•å…¥ä½æˆ·èˆ’é€‚åº¦åé¦ˆæœºåˆ¶å¯¹æ•´ä½“ç”µåŠ›è´Ÿè½½ä¸æ§åˆ¶æ€§èƒ½çš„å½±å“ã€‚
é€šè¿‡å¼•å…¥ä½æˆ·è¦†ç›–ï¼ˆoverrideï¼‰æ¨¡å‹ï¼Œæ¨¡æ‹Ÿå½“ç”¨æˆ·æ„Ÿåˆ°ä¸èˆ’é€‚æ—¶å›é€€æ’æ¸©å™¨è®¾å®šæ¸©åº¦çš„è¡Œä¸ºï¼ŒåŒæ—¶å¯¹æ¯”ä¸åŒå±‚æ¬¡çš„ä½æˆ·å»ºæ¨¡å¤æ‚åº¦ï¼ˆLevel of Detail, LoD 1â€“3ï¼‰å¯¹èƒ½æºæˆæœ¬ã€å³°å€¼è´Ÿè½½å‰Šå‡ä»¥åŠä½æˆ·ä¸é€‚æ¬¡æ•°çš„å½±å“

## ç¯å¢ƒé…ç½®

| æ¨¡å—               | é…ç½® / è¯´æ˜                                                                 |
| ------------------ | --------------------------------------------------------------------------- |
| **CityLearn ç‰ˆæœ¬** | 2.4.2ï¼ˆæˆ– â‰¥ 2.4ï¼‰                                                           |
| **Python ç‰ˆæœ¬**    | 3.10+                                                                       |
| **å¼ºåŒ–å­¦ä¹ ç®—æ³•**   | Soft Actorâ€“Critic (SAC)                                                     |
| **ç¡¬ä»¶ç¯å¢ƒ**       | Ubuntu/macOS, CPU / GPU å¯é€‰                                                |
| **åŸºç¡€æ•°æ®é›†**     | EULPï¼ˆElectric Utility Load Profiles, MontrÃ©al å†¬å­£ï¼‰                       |
| **ä½æˆ·è¡Œä¸ºæ¨¡å‹**   | Ecobee Donate Your Dataï¼ˆDYDï¼‰æ•°æ®è®­ç»ƒå‡ºçš„è¦†ç›–æ¦‚ç‡æ¨¡å‹                      |
| **ä»¿çœŸå‘¨æœŸ**       | å†¬å­£ä¸‰ä¸ªæœˆï¼ˆ12 æœˆâ€“2 æœˆï¼‰                                                    |
| **å»ºç­‘æ•°é‡**       | 10 æ ‹ç‹¬ç«‹ä½å®…                                                               |
| **ä»¿çœŸç²’åº¦**       | 1 å°æ—¶æ­¥é•¿ï¼ˆ24Ã—90 stepsï¼‰                                                   |
| **DR äº‹ä»¶**        | å·¥ä½œæ—¥ 18:00â€“21:00 è®¾å®šç‚¹ä¸‹è°ƒ 1.1Â°C (â‰ˆ2Â°F)                                  |
| **å¯¹ç…§ç»„**         | LoD1: åŸºçº¿ï¼Œæ— æ§åˆ¶ï¼›LoD2: Â±2Â°C èˆ’é€‚å¸¦ï¼ŒRL æ§åˆ¶ï¼›LoD3: Â±2Â°C+è¦†ç›–æ¨¡å‹+DR äº‹ä»¶ |
| **è¾“å‡ºæŒ‡æ ‡**       | ç”µè´¹æˆæœ¬ã€å³°å€¼åŠŸç‡ã€æ€»è€—ç”µé‡ã€è¦†ç›–æ¬¡æ•°ä¸è¦†ç›–å¹…åº¦                            |

## å¯¹æ¯”

| é¡¹ç›®             | 4.1 Basic Control  | 4.2 Community Coordination | 4.3 Multi-Objective (Economic/Emission) | **4.4 Comfort Feedback (æœ¬èŠ‚)**        |
| :--------------- | :----------------- | :------------------------- | :-------------------------------------- | :------------------------------------- |
| **ç ”ç©¶é‡ç‚¹**     | å•å»ºç­‘åŸºæœ¬è´Ÿè½½æ§åˆ¶ | å¤šå»ºç­‘é—´èƒ½é‡åè°ƒ           | å¤šç›®æ ‡æƒè¡¡ï¼ˆæˆæœ¬ vs æ’æ”¾ï¼‰              | ä½æˆ·èˆ’é€‚åé¦ˆä¸ DR å“åº”                 |
| **æ•°æ®é›†**       | ç®€åŒ–ä½å®…è´Ÿè½½       | ç¤¾åŒºçº§ EULP                | åŒä¸Š                                    | åŒä¸Šï¼ˆ+ DYD è¡Œä¸ºæ¨¡å‹ï¼‰                 |
| **æ§åˆ¶ç›®æ ‡**     | æˆæœ¬æœ€å°åŒ–         | æˆæœ¬ + è´Ÿè½½å¹³è¡¡            | æˆæœ¬ + æ’æ”¾                             | æˆæœ¬ + èˆ’é€‚åº¦ï¼ˆå‡å°‘è¦†ç›–æ¬¡æ•°ï¼‰          |
| **å¤–éƒ¨ä¿¡å·**     | ç”µä»·               | ç”µä»·                       | ç”µä»· + æ’æ”¾å› å­                         | ç”µä»· + DR äº‹ä»¶                         |
| **ä½æˆ·æ¨¡å‹**     | å›ºå®šè®¾å®šæ¸©åº¦       | å›ºå®šè®¾å®šæ¸©åº¦               | å›ºå®šè®¾å®šæ¸©åº¦                            | åŠ¨æ€è®¾å®šæ¸©åº¦ + è¦†ç›–è¡Œä¸º                |
| **LoD (å¤æ‚åº¦)** | LoD1               | LoD1â€“2                     | LoD1â€“2                                  | LoD1â€“3                                 |
| **å¼ºåŒ–å­¦ä¹ ç»“æ„** | å•æ™ºèƒ½ä½“ SAC       | å¤šæ™ºèƒ½ä½“ SAC               | å¤šç›®æ ‡ SAC (Reward = Î±Cost + Î²Emission) | å¤šç›®æ ‡ SAC (Reward = Î±Cost + Î²Comfort) |
| **è¯„ä»·æŒ‡æ ‡**     | èƒ½è€—/æˆæœ¬          | èƒ½è€—/å³°å€¼                  | æˆæœ¬+æ’æ”¾æŒ‡æ ‡                           | æˆæœ¬+å³°å€¼+è¦†ç›–æ¬¡æ•°                     |

æ€»ç»“æ¥è¯´ï¼Œ4.4 åœ¨ 4.3 åŸºç¡€ä¸ŠåŠ å…¥äººçš„å› ç´ ï¼ˆä½æˆ·èˆ’é€‚åé¦ˆï¼‰
