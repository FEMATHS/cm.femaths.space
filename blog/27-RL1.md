---
title: 'RL1 Preliminaries'
authors: [zqqqj]
tags: [RL]
date: 2025-12-20
---

# 强化学习1 基础

## 1.1 概述

懒的说了，自己百度一下什么都有，或者去看AI世界大入门去



<!-- truncate -->

## 1.2 基本定义

给出一些定义，这章没什么好看的，不亚于看一篇论文的时候看了半个小时intro。

- **强化学习（reinforcement learning，RL）**：智能体可以在与复杂且不确定的环境进行交互时，尝试使所获得的奖励最大化的算法。
- **动作（action）**： 环境接收到的智能体基于当前状态的输出。
- **状态（state）**：智能体从环境中获取的状态。
- **奖励（reward）**：智能体从环境中获取的反馈信号，这个信号指定了智能体在某一步采取了某个策略以后是否得到奖励，以及奖励的大小。
- **探索（exploration）**：在当前的情况下，继续尝试新的动作。其有可能得到更高的奖励，也有可能一无所有。
- **利用（exploitation）**：在当前的情况下，继续尝试已知的可以获得最大奖励的过程，即选择重复执行当前动作。
- **深度强化学习（deep reinforcement learning）**：不需要手动设计特征，仅需要输入状态就可以让系统直接输出动作的一个端到端（end-to-end）的强化学习方法。通常使用神经网络来拟合价值函数（value function）或者策略网络（policy network）。
- **全部可观测（full observability）、完全可观测（fully observed）和部分可观测（partially observed）**：当智能体的状态与环境的状态等价时，我们就称这个环境是全部可观测的；当智能体能够观察到环境的所有状态时，我们称这个环境是完全可观测的；一般智能体不能观察到环境的所有状态时，我们称这个环境是部分可观测的。
- **部分可观测马尔可夫决策过程（partially observable Markov decision process，POMDP）**：即马尔可夫决策过程的泛化。部分可观测马尔可夫决策过程依然具有马尔可夫性质，但是其假设智能体无法感知环境的状态，只能知道部分观测值。
- **动作空间（action space）、离散动作空间（discrete action space）和连续动作空间（continuous action space）**：在给定的环境中，有效动作的集合被称为动作空间，智能体的动作数量有限的动作空间称为离散动作空间，反之，则被称为连续动作空间。
- **基于策略的（policy-based）**：智能体会制定一套动作策略，即确定在给定状态下需要采取何种动作，并根据这个策略进行操作。强化学习算法直接对策略进行优化，使制定的策略能够获得最大的奖励。
- **基于价值的（valued-based）**：智能体不需要制定显式的策略，它维护一个价值表格或者价值函数，并通过这个价值表格或价值函数来执行使得价值最大化的动作。
- **有模型（model-based）结构**：智能体通过学习状态的转移来进行决策。
- **免模型（model-free）结构**：智能体没有直接估计状态的转移，也没有得到环境的具体转移变量，它通过学习价值函数或者策略网络进行决策。
